{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lending Club Analysis Using AutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup H2O Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.init(max_mem_size = \"6g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data and Manage Data Types\n",
    "\n",
    "This exploration of H2O will use a version of the Lending Club Loan Data that can be found on [Kaggle](https://www.kaggle.com/wendykan/lending-club-loan-data). This data consists of 15 variables:\n",
    "\n",
    "|     | Column Name | Description |\n",
    "| --- | ----------- | ----------- |\n",
    "|   1 | loan_amnt   | Requested loan amount (US dollars) |\n",
    "|   2 | term        | Loan term length (months) |\n",
    "|   3 | int_rate    | Recommended interest rate |\n",
    "|   4 | emp_length  | Employment length (years) |\n",
    "|   5 | home_ownership| Housing status |\n",
    "|   6 | annual_inc  | Annual income (US dollars) |\n",
    "|   7 | purpose     | Purpose for the loan |\n",
    "|   8 | addr_state  | State of residence |\n",
    "|   9 | dti         | Debt to income ratio |\n",
    "|  10 | delinq_2yrs | Number of delinquencies in the past 2 years |\n",
    "|  11 | revol_util  | Percent of revolving credit line utilized |\n",
    "|  12 | total_acc   | Number of active accounts |\n",
    "|  13 | bad_loan    | Bad loan indicator |\n",
    "|  14 | longest_credit_length | Age of oldest active account |\n",
    "|  15 | verification_status | Income verification status |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://s3-us-west-2.amazonaws.com/h2o-tutorials/data/topics/data/automl/loan.csv\n",
    "train = h2o.import_file(\"../../data/topics/automl/loan.csv\")\n",
    "train[\"bad_loan\"] = train[\"bad_loan\"].asfactor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models Using H2O's AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set target and predictor variables\n",
    "y = \"bad_loan\"\n",
    "x = train.col_names\n",
    "x.remove(y)\n",
    "x.remove(\"int_rate\")\n",
    "\n",
    "# Use Auto ML to train models\n",
    "from h2o.automl import H2OAutoML\n",
    "aml = H2OAutoML(max_models = 6, exclude_algos = ['DeepLearning'])\n",
    "aml.train(x = x, y = y, training_frame = train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aml.leaderboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = h2o.get_model(aml.leaderboard[2,'model_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "import matplotlib.cbook\n",
    "warnings.filterwarnings(\"ignore\", category = matplotlib.cbook.mplDeprecation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.varimp_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"AUC: train = {:.4f}, xval = {:.4f}\"\\\n",
    "      .format(best_model.auc(train = True), best_model.auc(xval = True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logloss: train = {:.4f}, xval = {:.4f}\" \\\n",
    "      .format(best_model.logloss(train = True), best_model.logloss(xval = True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Feature Engineering\n",
    "\n",
    "The goal of this section is to improve upon these predictors through a number of feature engineering steps. In particular, we will perform three feature engineering tasks:\n",
    "\n",
    "1. **Separating Typical from Extreme Loan Amount**\n",
    "1. **Converting Term to a 0/1 Indicator**\n",
    "1. **Creating Missing Value Indicator for Employment Length**\n",
    "1. **Combining Categories in Home Ownership**\n",
    "1. **Separating Typical from Extreme Annual Income**\n",
    "1. **Creating Target Encoding for Loan Purpose**\n",
    "1. **Creating Target Encoding for State of Residence**\n",
    "1. **Separating Typical from Extreme Debt to Income Ratio**\n",
    "1. **Separating Typical from Extreme Number of Delinquencies in the Past 2 Years**\n",
    "1. **Separating Typical from Extreme Revolving Credit Line Utilized**\n",
    "1. **Separating Typical from Extreme Number of Credit Lines**\n",
    "1. **Separating Typical from Extreme Longest Credit Length**\n",
    "1. **Converting Income Verification Status to a 0/1 Indicator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = \"bad_loan\"\n",
    "x_orig = train.col_names\n",
    "x_orig.remove(y)\n",
    "x_orig.remove(\"int_rate\")\n",
    "\n",
    "x_trans = x_orig.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation and Target Encoding\n",
    "\n",
    "Some of the engineered features will use [cross-validated mean target encoding](https://github.com/h2oai/h2o-tutorials/blob/master/best-practices/categorical-predictors/target_encoding.md) of categorical predictors since one hot encodings can lead to overfitting of infrequent categories.\n",
    "\n",
    "To achieve this goal, we will first create soft partitions using H2OFrame's [`kfold_column`](http://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/frame.html?highlight=kfold_column#h2o.frame.H2OFrame.kfold_column) function, then calculate summary statistics using H2O's [`group_by`](http://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/frame.html#groupby) function, and finally join these engineered features using H2OFrame's [`merge`](http://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/frame.html?highlight=merge#h2o.frame.H2OFrame.merge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_nfolds = 5\n",
    "cv_seed = 2307"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"cv_fold\"] = train.kfold_column(n_folds = cv_nfolds, seed = cv_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"cv_fold\"].table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(p):\n",
    "    return np.log(p) - np.log(1 - p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_target(data, x, y = \"bad_loan\"):\n",
    "    grouped_data = data[[x, y]].group_by([x])\n",
    "    stats = grouped_data.count(na = \"ignore\").mean(na = \"ignore\")\n",
    "    return stats.get_frame().as_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_target_encoding(data, x, y = \"bad_loan\", fold_column = \"cv_fold\", prior_mean = 0.183, prior_count = 1):\n",
    "    \"\"\"\n",
    "    Creates target encoding for binary target\n",
    "    data (H2OFrame) : data set\n",
    "    x (string) : categorical predictor column name\n",
    "    y (string) : binary target column name\n",
    "    fold_column (string) : cross-validation fold column name\n",
    "    prior_mean (float) : proportion of 1s in the target column\n",
    "    prior_count (positive number) : weight to give to prior_mean\n",
    "    \"\"\" \n",
    "    grouped_data = data[[x, fold_column, y]].group_by([x, fold_column])\n",
    "    grouped_data.sum(na = \"ignore\").count(na = \"ignore\")\n",
    "    df = grouped_data.get_frame().as_data_frame()\n",
    "    df_list = []\n",
    "    nfold = int(data[fold_column].max()) + 1\n",
    "    for j in range(0, nfold):\n",
    "        te_x = \"te_{}\".format(x)\n",
    "        sum_y = \"sum_{}\".format(y)\n",
    "        oof = df.loc[df[fold_column] != j, [x, sum_y, \"nrow\"]]\n",
    "        stats = oof.groupby([x]).sum()\n",
    "        stats[x] = stats.index\n",
    "        stats[fold_column] = j\n",
    "        p = (stats[sum_y] + (prior_count * prior_mean)) / (stats[\"nrow\"] + prior_count)\n",
    "        stats[te_x] = logit(p)\n",
    "        df_list.append(stats[[x, fold_column, te_x]])\n",
    "    return h2o.H2OFrame(pd.concat(df_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating Typical from Extreme Loan Amount\n",
    "\n",
    "After binning `loan_amt` using H2OFrame's [`cut`](http://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/frame.html?highlight=cut#h2o.frame.H2OFrame.cut) function and looking at the fraction of bad loans on a logit scale, we see that the chance of a bad loan roughly increases linearly in loan amount from \\\\$5,000 to \\\\$30,000 and is relatively flat below \\$5,000 and above \\$30,000. To reflect this finding in the modeling, we will replace the original `loan_amnt` measure with two derived measures:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "loan\\_amnt\\_core & = & \\max(5000, \\min(loan\\_amnt, 30000)) \\\\\n",
    "loan\\_amnt\\_diff & = & loan\\_amnt - loan\\_amnt\\_core\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"loan_amnt\"].quantile([0, 0.05, 0.25, 0.5, 0.75, 0.95, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breaks = np.linspace(0, 35000, 8).tolist()\n",
    "train[\"loan_amnt_cat\"] = train[\"loan_amnt\"].cut(breaks = breaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mean_target(train, \"loan_amnt_cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xticks(rotation = 90)\n",
    "plt.yscale(\"logit\")\n",
    "plt.plot(df[\"loan_amnt_cat\"], df[\"mean_bad_loan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trans.remove(\"loan_amnt\")\n",
    "x_trans.append(\"loan_amnt_core\")\n",
    "x_trans.append(\"loan_amnt_delta\")\n",
    "\n",
    "train[\"loan_amnt_core\"] = h2o.H2OFrame.ifelse(train[\"loan_amnt\"] <= 5000, 5000, train[\"loan_amnt\"])\n",
    "train[\"loan_amnt_core\"] = h2o.H2OFrame.ifelse(train[\"loan_amnt_core\"] <= 30000, train[\"loan_amnt_core\"], 30000)\n",
    "\n",
    "train[\"loan_amnt_delta\"] = train[\"loan_amnt\"] - train[\"loan_amnt_core\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Term to a 0/1 Indicator\n",
    "\n",
    "Given that term of the loans are either 3 or 5 years, we will create a simplifed `term_36month` binary indicator that is 1 when the terms of the loan is for 5 years and 0 for loans with a term of 3 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"term\"].table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trans.remove(\"term\")\n",
    "x_trans.append(\"term_60months\")\n",
    "\n",
    "train[\"term_60months\"] = train[\"term\"] == \"60 months\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"term_60months\"].table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Missing Value Indicator for Employment Length\n",
    "\n",
    "The most interesting characteristic about employment length is whether or not it is missing. The divide between those with missing values for employment length to those who have a recorded employment length is 26.3% bad loans to 18.0% bad loans respectively. Interestingly, there doesn't appear to be any differences in bad loans across employment lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"emp_length\"].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trans.append(\"emp_length_missing\")\n",
    "\n",
    "train[\"emp_length_missing\"] = train[\"emp_length\"] == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_target_encoding(train, \"emp_length_missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mean_target(train, \"emp_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.yscale(\"logit\")\n",
    "plt.plot(df[\"emp_length\"], df[\"mean_bad_loan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Categories in Home Ownership\n",
    "\n",
    "Although there are 6 recorded categories within home ownership, only three had over 200 observations: OWN, MORTGAGE, and RENT. The remaining three are so infrequent we will combine them {ANY, NONE, OTHER} with RENT to form an enlarged OTHER category. This new `home_ownership_3cat` variable will have values in {MORTGAGE, OTHER, OWN}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_target(train, \"home_ownership\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvls = [\"OTHER\", \"MORTGAGE\", \"OTHER\", \"OTHER\", \"OWN\", \"OTHER\"]\n",
    "train[\"home_ownership_3cat\"] = train[\"home_ownership\"].set_levels(lvls).ascharacter().asfactor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[[\"home_ownership\", \"home_ownership_3cat\"]].table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_target(train, \"home_ownership_3cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trans.remove(\"home_ownership\")\n",
    "x_trans.append(\"home_ownership_3cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating Typical from Extreme Annual Income\n",
    "\n",
    "Looking at the occurance of bad loans on a logit scale reveal that the chance of a bad loan roughly decreases linearly in annual income from \\$10,000 to \\$105,000 and is relatively flat above \\$105,000. To reflect this finding in the modeling, we will replace the original `annual_inc` measure with two derived measures:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "annual\\_inc\\_core & = & \\max(10000, \\min(annual\\_inc, 105000)) \\\\\n",
    "annual\\_inc\\_diff & = & annual\\_inc - annual\\_inc\\_core\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"annual_inc\"].quantile([0, 0.05, 0.25, 0.5, 0.75, 0.95, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breaks = np.linspace(0, 150000, 31).tolist()\n",
    "train[\"annual_inc_cat\"] = train[\"annual_inc\"].cut(breaks = breaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mean_target(train, \"annual_inc_cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.yscale(\"logit\")\n",
    "plt.plot(df[\"annual_inc_cat\"].index, df[\"mean_bad_loan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[20:31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trans.remove(\"annual_inc\")\n",
    "x_trans.append(\"annual_inc_core\")\n",
    "x_trans.append(\"annual_inc_delta\")\n",
    "\n",
    "train[\"annual_inc_core\"] = h2o.H2OFrame.ifelse(train[\"annual_inc\"] <= 10000, 10000, train[\"annual_inc\"])\n",
    "train[\"annual_inc_core\"] = h2o.H2OFrame.ifelse(train[\"annual_inc_core\"] <= 105000,\n",
    "                                               train[\"annual_inc_core\"], 105000)\n",
    "\n",
    "train[\"annual_inc_delta\"] = train[\"annual_inc\"] - train[\"annual_inc_core\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Target Encoding for Loan Purpose\n",
    "\n",
    "Given that there is a high concentration of loans for debt consolidation (56.87%), a sizable number for credit card (18.78%), and the remaining 24.35% loans are spread amongst 12 other purposes, we will use mean target encoding to avoid overfitting of the later group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl = train[\"purpose\"].table().as_data_frame()\n",
    "tbl[\"Percent\"] = np.round((100 * tbl[\"Count\"]/train.nrows), 2)\n",
    "tbl = tbl.sort_values(by = \"Count\", ascending = 0)\n",
    "tbl = tbl.reset_index(drop = True)\n",
    "print(tbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mean_target(train, \"purpose\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by = \"mean_bad_loan\", ascending = 0)\n",
    "df = df.reset_index(drop = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xticks(rotation = 90)\n",
    "plt.yscale(\"logit\")\n",
    "plt.plot(df[\"purpose\"], df[\"mean_bad_loan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_purpose = mean_target_encoding(train, \"purpose\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(te_purpose, all_x = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trans.remove(\"purpose\")\n",
    "x_trans.append(\"te_purpose\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"te_purpose\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Target Encoding for State of Residence\n",
    "\n",
    "We will also use a mean target encoding for state of residence for a reason similar to that for purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl = train[\"addr_state\"].table().as_data_frame()\n",
    "tbl[\"Percent\"] = np.round((100 * tbl[\"Count\"]/train.nrows), 2)\n",
    "tbl = tbl.sort_values(by = \"Count\", ascending = 0)\n",
    "tbl = tbl.reset_index(drop = True)\n",
    "print(tbl[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mean_target(train, \"addr_state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by = \"mean_bad_loan\", ascending = 0)\n",
    "df = df.reset_index(drop = True)\n",
    "print(df[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[45:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.yscale(\"logit\")\n",
    "plt.plot(df[\"addr_state\"], df[\"mean_bad_loan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_addr_state = mean_target_encoding(train, \"addr_state\", prior_count = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = te_addr_state[[\"addr_state\", \"te_addr_state\"]].group_by([\"addr_state\"])\n",
    "df = grouped_data.count(na = \"ignore\").mean(na = \"ignore\").get_frame().as_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by = \"mean_te_addr_state\", ascending = 0)\n",
    "df = df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df[\"addr_state\"], df[\"mean_te_addr_state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(te_addr_state, all_x = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trans.remove(\"addr_state\")\n",
    "x_trans.append(\"te_addr_state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"te_addr_state\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating Typical from Extreme Debt to Income Ratio\n",
    "\n",
    "Looking at the occurance of bad loans on a logit scale reveal that the chance of a bad loan roughly increases linearly in debt-to-income from 5% to 30% and is highly volatile outside of that range due to small numbers of observations. To reflect this finding in the modeling, we will replace the original `dti` measure with two derived measures:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "dti\\_core & = & \\max(5, \\min(dti, 30)) \\\\\n",
    "dti\\_diff & = & dti - dti\\_core\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"dti\"].quantile([0, 0.05, 0.25, 0.5, 0.75, 0.95, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breaks = np.linspace(0, 40, 41).tolist()\n",
    "train[\"dti_cat\"] = train[\"dti\"].cut(breaks = breaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mean_target(train, \"dti_cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.yscale(\"logit\")\n",
    "plt.plot(df[\"dti_cat\"].index, df[\"mean_bad_loan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[30:41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trans.remove(\"dti\")\n",
    "x_trans.append(\"dti_core\")\n",
    "x_trans.append(\"dti_delta\")\n",
    "\n",
    "train[\"dti_core\"] = h2o.H2OFrame.ifelse(train[\"dti\"] <= 5, 5, train[\"dti\"])\n",
    "train[\"dti_core\"] = h2o.H2OFrame.ifelse(train[\"dti_core\"] <= 30, train[\"dti_core\"], 30)\n",
    "\n",
    "train[\"dti_delta\"] = train[\"dti\"] - train[\"dti_core\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating Typical from Extreme Number of Delinquencies in the Past 2 Years\n",
    "\n",
    "The chance of a bad loan seems to max out at 3 delinquent payments in the past two years. To reflect this finding in the modeling, we will replace the original `delinq_2yrs` measure with two derived measures:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "delinq\\_2yrs\\_core & = & \\min(delinq\\_2yrs, 3) \\\\\n",
    "delinq\\_2yrs\\_diff & = & delinq\\_2yrs - delinq\\_2yrs\\_core\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"delinq_2yrs\"].quantile([0, 0.05, 0.25, 0.5, 0.75, 0.95, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breaks = np.linspace(0, 5, 6).tolist()\n",
    "train[\"delinq_2yrs_cat\"] = train[\"delinq_2yrs\"].cut(breaks = breaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_target(train, \"delinq_2yrs_cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trans.remove(\"delinq_2yrs\")\n",
    "x_trans.append(\"delinq_2yrs_core\")\n",
    "x_trans.append(\"delinq_2yrs_delta\")\n",
    "\n",
    "train[\"delinq_2yrs_core\"] = h2o.H2OFrame.ifelse(train[\"delinq_2yrs\"] <= 3, train[\"delinq_2yrs\"], 3)\n",
    "\n",
    "train[\"delinq_2yrs_delta\"] = train[\"delinq_2yrs\"] - train[\"delinq_2yrs_core\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating Typical from Extreme Revolving Credit Line Utilized\n",
    "\n",
    "The relationship between credit line utilized is somewhat interesting. There appears to be a higher rate for a bad loan when 0% of the credit lines are utilized, then it drops down slightly and roughly increases linearly in credit line utilized up to 100%. To reflect this finding in the modeling, we will replace the original `revol_util` measure with three derived measures:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "revol\\_util\\_0 & = & I(revol\\_util == 0) \\\\\n",
    "revol\\_util\\_core & = & \\max(5, \\min(revol\\_util, 30)) \\\\\n",
    "revol\\_util\\_diff & = & revol\\_util - revol\\_util\\_core\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"revol_util\"].quantile([0, 0.05, 0.25, 0.5, 0.75, 0.95, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breaks = np.linspace(0, 120, 25).tolist()\n",
    "train[\"revol_util_cat\"] = train[\"revol_util\"].cut(breaks = breaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mean_target(train, \"revol_util_cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.yscale(\"logit\")\n",
    "plt.plot(df[\"revol_util_cat\"].index, df[\"mean_bad_loan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[20:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trans.remove(\"revol_util\")\n",
    "x_trans.append(\"revol_util_0\")\n",
    "x_trans.append(\"revol_util_core\")\n",
    "x_trans.append(\"revol_util_delta\")\n",
    "\n",
    "train[\"revol_util_0\"] = train[\"revol_util\"] == 0\n",
    "\n",
    "train[\"revol_util_core\"] = h2o.H2OFrame.ifelse(train[\"revol_util\"] <= 100, train[\"revol_util\"], 100)\n",
    "\n",
    "train[\"revol_util_delta\"] = train[\"revol_util\"] - train[\"revol_util_core\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating Typical from Extreme Number of Credit Lines\n",
    "\n",
    "Looking at the occurance of bad loans on a logit scale reveal that the chance of a bad loan roughly decreases linearly in number of lines of credit up to about 50. To reflect this finding in the modeling, we will replace the original `total_acc` measure with two derived measures:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "total\\_acc\\_core & = & \\min(total\\_acc, 50) \\\\\n",
    "total\\_acc\\_diff & = & total\\_acc - total\\_acc\\_core\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"total_acc\"].quantile([0, 0.05, 0.25, 0.5, 0.75, 0.95, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breaks = np.linspace(0, 60, 13).tolist()\n",
    "train[\"total_acc_cat\"] = train[\"total_acc\"].cut(breaks = breaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mean_target(train, \"total_acc_cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.yscale(\"logit\")\n",
    "plt.plot(df[\"total_acc_cat\"].index, df[\"mean_bad_loan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train[\"total_acc\"] == None).table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[8:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trans.remove(\"total_acc\")\n",
    "x_trans.append(\"total_acc_core\")\n",
    "x_trans.append(\"total_acc_delta\")\n",
    "\n",
    "train[\"total_acc_core\"] = h2o.H2OFrame.ifelse(train[\"total_acc\"] <= 50, train[\"total_acc\"], 50)\n",
    "\n",
    "train[\"total_acc_delta\"] = train[\"total_acc\"] - train[\"total_acc_core\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating Typical from Extreme Longest Credit Length\n",
    "\n",
    "Looking at the occurance of bad loans on a logit scale reveal that the chance of a bad loan roughly decreases linearly in longest credit length from 3 to 20 years and is highly volatile outside of that range due to small numbers of observations. To reflect this finding in the modeling, we will replace the original `longest_credit_length` measure with two derived measures:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "longest\\_credit\\_length\\_core & = & \\max(3, \\min(longest\\_credit\\_length, 20)) \\\\\n",
    "longest\\_credit\\_length\\_diff & = & longest\\_credit\\_length - longest\\_credit\\_length\\_core\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"longest_credit_length\"].quantile([0, 0.05, 0.25, 0.5, 0.75, 0.95, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breaks = np.linspace(0, 25, 26).tolist()\n",
    "train[\"longest_credit_length_cat\"] = train[\"longest_credit_length\"].cut(breaks = breaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mean_target(train, \"longest_credit_length_cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.yscale(\"logit\")\n",
    "plt.plot(df[\"longest_credit_length_cat\"].index, df[\"mean_bad_loan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[20:26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trans.remove(\"longest_credit_length\")\n",
    "x_trans.append(\"longest_credit_length_core\")\n",
    "x_trans.append(\"longest_credit_length_delta\")\n",
    "\n",
    "train[\"longest_credit_length_core\"] = h2o.H2OFrame.ifelse(train[\"longest_credit_length\"] <= 3,\n",
    "                                                          3, train[\"longest_credit_length\"])\n",
    "train[\"longest_credit_length_core\"] = h2o.H2OFrame.ifelse(train[\"longest_credit_length_core\"] <= 20,\n",
    "                                                          train[\"longest_credit_length_core\"], 20)\n",
    "\n",
    "train[\"longest_credit_length_delta\"] = train[\"longest_credit_length\"] - train[\"longest_credit_length_core\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Income Verification Status to a 0/1 Indicator\n",
    "\n",
    "Given that incomes are either verified or not verified, we will create a simplifed `verified` binary indicator that is 1 when income has been verified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"verification_status\"].table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trans.remove(\"verification_status\")\n",
    "x_trans.append(\"verified\")\n",
    "\n",
    "train[\"verified\"] = train[\"verification_status\"] == \"verified\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"verified\"].table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models Using Transformed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aml.leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml_trans = H2OAutoML(max_models = 6, exclude_algos = ['DeepLearning'])\n",
    "aml_trans.train(x = x_trans, y = y, training_frame = train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aml_trans.leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_trans = h2o.get_model(aml_trans.leaderboard[2,'model_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_trans.varimp_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUC (orig): train = {:.4f}, xval = {:.4f}\" \\\n",
    "      .format(best_model.auc(train = True),\n",
    "              best_model.auc(xval = True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUC (trans): train = {:.4f}, xval = {:.4f}\" \\\n",
    "      .format(best_model_trans.auc(train = True),\n",
    "              best_model_trans.auc(xval = True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logloss (orig): train = {:.4f}, xval = {:.4f}\" \\\n",
    "      .format(best_model.logloss(train = True),\n",
    "              best_model.logloss(xval = True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logloss (trans): train = {:.4f}, xval = {:.4f}\" \\\n",
    "      .format(best_model_trans.logloss(train = True),\n",
    "              best_model_trans.logloss(xval = True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shutdown H2O Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.cluster().shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
